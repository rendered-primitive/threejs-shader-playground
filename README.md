Hello Peter -> 

Here is a repo where I've been testing out various pipelines for doing effect composition, depth testing etc. 

There is an excellent article here on depth testing, which converts the rendered screen into a a representation of the world position of pixels. It does this by using the depth buffer, and screen position of pixels. 

https://www.thefrontdev.co.uk/post-processing-in-react-three-fiber-depth-textures-and-world-coordinates-in-fragment-shaders

The next challenge is extracting the pixel colour from under the mouse. There are a few ways of doing this, but I've yet to yet achieve it. Not sure what's going wrong...

